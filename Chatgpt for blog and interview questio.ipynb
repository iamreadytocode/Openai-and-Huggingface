{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11742,"status":"ok","timestamp":1707039195171,"user":{"displayName":"Muhammad Abdulllah","userId":"14925103515442766550"},"user_tz":-300},"id":"P-_FoymRlrv2","outputId":"cb725ff0-a48e-4f71-d335-247d758cbb7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting openai\n","  Downloading openai-1.11.0-py3-none-any.whl (226 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.14)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n","Collecting typing-extensions<5,>=4.7 (from openai)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.11.0 typing-extensions-4.9.0\n"]}],"source":["!pip install openai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SKu9WP6Il_0l"},"outputs":[],"source":["import openai\n","\n","openai.api_key=\"\"\n","\n","def Senti_analysis(text):\n","  messages=[    {\"role\":\"system\",\"content\":\"\"\"You are to perform the function of determinning the sentiment of the given text.\n","                                    If you are not sure ask the user to determine the statement mannually\"\"\"},\n","      {\"role\":\"user\",\"content\":f\"\"\"You are to determone the sentimental analysis of the given text snd give\n","                               a single word answer about the given statement either positive or negative:{text} \"\"\"}\n","  ]\n","  response=openai.chat.completions.create(model=\"gpt-3.5-turbo\",\n","                                          messages=messages,\n","                                          max_tokens=1,\n","                                          temperature=0,\n","                                          n=1)\n","  response_text = response.choices[0].message.content.strip().lower()\n","\n","  return response_text\n","\n","\n","prompt=input(\"Enter your prompt \")\n","response=Senti_analysis(prompt)\n","print(prompt,\": The statement is \",response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XceBXAcnZb1K"},"outputs":[],"source":["def generate_blog(topic):\n","  messages= [\n","      {\"role\": \"system\", \"content\": \"\"\"You are trained to analyze a topic and generate a blog post.\n","                                        The blog post must contain 1500 to 3000 words (No less than 1500 words).\"\"\"},\n","        {\"role\": \"user\", \"content\": f\"\"\"Analyze the topic and generate a blog post. The topic is {topic}\n","                                        The blog post should contain the following format.\n","                                        1) Title (Not more than one line).\n","                                        2) Introduction (Give introducion about the topic)\n","                                        3) Add an image url relevent to the topic.\n","                                        4) Add 2/3 subheadings and explain them.\n","                                        5) Body (should describe the facts and findings)\n","                                        6) Add an image url relevent to the topic.\n","                                        7) Add 2/3 subheadings and explain them.\n","                                        8) General FAQ regarding the topic.\n","                                        9) Conclusion of the topic. \"\"\"}\n","  ]\n","  response = openai.chat.completions.create(\n","      model=\"gpt-3.5-turbo\",\n","      messages=messages,\n","      max_tokens=100,\n","      n=1,\n","      temperature=0.5\n","  )\n","\n","  response_text= response.chooice[0].message.content.strip().lower()\n","\n","  return response_text\n","\n","user_input=input(\"Enter the topic here\")\n","blog=generate_blog(user_input)\n","print(blog)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ts7xf8-GKue"},"outputs":[],"source":["import io\n","import openai\n","import PIL\n","import requests\n","from PIL import Image\n","\n","openai.api_key=\"\"\n","\n","def gen_image(text):\n","  \"\"\"\n","  Generate images using Openai's DALL-E model according to the given pompt.\n","\n","  :param text: the prompt to generate the image.\n","  :return PIL image object.\n","  \"\"\"\n","  no_img=1\n","  response=openai.images.generate(\n","      prompt=text,\n","      n=no_img,\n","      size=\"256*256\"\n","  )\n","  img_url= response.data[0].url\n","\n","  image_content= requests.get(img_url).content\n","  image = Image.open(io.BytesIO(image_content))\n","\n","  return image\n","\n","prompt = input(\"Enter the prompt for image generation .\")\n","image=gen_image(prompt)\n","image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QqUZrquOi0b0"},"outputs":[],"source":["import io\n","import openai\n","import PIL\n","import requests\n","from PIL import Image\n","\n","openai.api_key=\"\"\n","\n","def gen_image(text):\n","  \"\"\"\n","  Generate images using Openai's DALL-E model according to the given pompt.\n","\n","  :param text: the prompt to generate the image.\n","  :return PIL image object.\n","  \"\"\"\n","  no_img=5\n","  responses=openai.images.generate(\n","      prompt=text,\n","      n=no_img,\n","      size=\"256*256\"\n","  )\n","  images=[]\n","  for response in responses.data:\n","    image_url=response.url\n","    image_content= requests.get(image_url).content\n","    image = Image.open(io.BytesIO(image_content))\n","    images.append(image)\n","  return images\n","\n","prompt = input(\"Enter the prompt for image generation .\")\n","image=gen_image(prompt)\n","image\n","\n","from IPython.display import display\n","for image in images:\n","  display(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9NAkG1MvMby"},"outputs":[],"source":["import io\n","import openai\n","import requests\n","import PIL\n","from PIL import Image\n","\n","openai.api_key = ''\n","\n","def generate_image_with_text_prompt_dall_e_3(text_prompt):\n","    \"\"\"\n","    Generate an image using OpenAI's DALL-E model based on the provided text prompt.\n","\n","    :param text_prompt: The prompt to generate the image.\n","    :return: PIL Image object.\n","    \"\"\"\n","    number_of_images = 1\n","    # Generate the image using OpenAI's DALL-E model\n","    response = openai.images.generate(\n","        model=\"dall-e-3\",\n","        prompt=text_prompt,\n","        n=number_of_images,\n","        size=\"1024x1024\"\n","    )\n","\n","    # Get the image URL from the response\n","    image_url = response.data[0].url\n","\n","    # Download the image content and convert it to a PIL image\n","    image_content = requests.get(image_url).content\n","    image = Image.open(io.BytesIO(image_content))\n","\n","\n","    return image\n","\n","prompt = input(\"Enter the prompt for image generation .\")\n","image=generate_image_with_text_prompt_dall_e_3(prompt)\n","image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FigoGOciEZHF"},"outputs":[],"source":["import openai\n","openai.api_key=\"\"\n","\n","def interview_question(topic):\n","  global messages\n","  messages=[{\"role\":\"system\",\"content\":\"\"\"You are to generate 8 questions about the topic that I will give you.\n","                            Generate 8 questions based on the provided prompt.\"\"\"},\n","      {\n","      \"role\": \"user\",\n","      \"content\": f\"Create a list of 8 questions for an interview on a given topic{topic}.\"\n","    }\n","  ]\n","\n","  response = openai.chat.completions.create(\n","           model=\"gpt-3.5-turbo\",\n","           messages=messages,\n","           temperature=0.5,\n","           max_tokens=80,\n","           top_p=1\n","            )\n","  response_questions=response.choices[0].message.content.strip().lower()\n","  return response_questions\n","\n","prompt=input(\"enter the topic on which u want interview questions \")\n","response=interview_question(prompt)\n","print(prompt , response)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOrvRuBeCYceUhWd2vzAkzp","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"}}},"nbformat":4,"nbformat_minor":0}
